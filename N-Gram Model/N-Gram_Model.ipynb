{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d97e49d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus saved as: C:\\Users\\sunka\\OneDrive\\Desktop\\VI Sem\\CS374 NLP\\Lab\\BigramModel\\N-Gram Model\\data\\Arthur_Conan_Doyle_corpus.txt\n",
      "Total characters: 2963014\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_gutenberg_text(text):\n",
    "    # Remove header\n",
    "    start_pattern = r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\"\n",
    "    end_pattern   = r\"\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\"\n",
    "\n",
    "    text = re.sub(start_pattern, \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(end_pattern, \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "paths = [\n",
    "    r\".\\data\\Adventures of Sherlock Holmes.txt\",\n",
    "    r\".\\data\\Memories and Adventures.txt\",\n",
    "    r\".\\data\\The case-book of Sherlock Holmes.txt\",\n",
    "    r\".\\data\\The Memoirs of Sherlock Holmes.txt\",\n",
    "    r\".\\data\\The Return of Sherlock Holmes.txt\"\n",
    "]\n",
    "\n",
    "# ---------- Read, clean, and merge ----------\n",
    "cleaned_texts = []\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "        cleaned_text = clean_gutenberg_text(raw_text)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "final_corpus = \"\\n\".join(cleaned_texts)\n",
    "\n",
    "output_path = r\"C:\\Users\\sunka\\OneDrive\\Desktop\\VI Sem\\CS374 NLP\\Lab\\BigramModel\\N-Gram Model\\data\\Arthur_Conan_Doyle_corpus.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_corpus)\n",
    "\n",
    "print(\"Corpus saved as:\", output_path)\n",
    "print(\"Total characters:\", len(final_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac6c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 632023\n",
      "Sample tokens: ['adventures', 'of', 'sherlock', 'holmes', 'adventure', 'i', 'a', 'scandal', 'in', 'bohemia', 'i', 'to', 'sherlock', 'holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other']\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\sunka\\OneDrive\\Desktop\\VI Sem\\CS374 NLP\\Lab\\BigramModel\\N-Gram Model\\data\\Arthur_Conan_Doyle_corpus.txt\"\n",
    ",\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "# Tokenize into words and punctuation\n",
    "tokens = re.findall(r\"\\b\\w+\\b|[.,!?;]\", corpus)\n",
    "\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Sample tokens:\", tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "844a955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5-grams: 632023\n",
      "Sample 5-gram: ('a', 'scandal', 'in', 'bohemia', 'i')\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "pad_token = \"<s>\"\n",
    "\n",
    "tokens = [pad_token] * (N - 1) + tokens\n",
    "\n",
    "five_grams = []\n",
    "\n",
    "for i in range(len(tokens) - N + 1):\n",
    "    five_grams.append(tuple(tokens[i:i+N]))\n",
    "\n",
    "print(\"Total 5-grams:\", len(five_grams))\n",
    "print(\"Sample 5-gram:\", five_grams[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908b7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 5-grams: 610706\n",
      "Unique contexts (4-grams): 558656\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "fivegram_counts = defaultdict(int)\n",
    "context_counts = defaultdict(int)\n",
    "\n",
    "for gram in five_grams:\n",
    "    context = gram[:-1]   # first 4 words\n",
    "    target = gram[-1]     # 5th word\n",
    "\n",
    "    fivegram_counts[gram] += 1\n",
    "    context_counts[context] += 1\n",
    "\n",
    "print(\"Unique 5-grams:\", len(fivegram_counts))\n",
    "print(\"Unique contexts (4-grams):\", len(context_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4646c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivegram_probability(context, word):\n",
    "    gram = context + (word,)\n",
    "    return fivegram_counts[gram] / context_counts[context]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a603df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we arrived at baker street late in the evening , and he tossed it across to me . it was a very bad day in which i failed to entirely avoid . the next few minutes were delicious . it was a very bad day\n",
      "holmes looked at me thoughtfully and shook his head . i am afraid that i must leave you to your papers for a little . you appear to take it for granted that , although the door was forced , the robber never got\n",
      "i could not believe it to be true . i had no idea that the case was serious , for i had a fit in the station , and we were able to find out where it was . evans had indeed done great\n",
      "he said nothing but smiled\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "context_to_candidates = defaultdict(set)\n",
    "\n",
    "for gram in fivegram_counts:\n",
    "    context = gram[:-1]   # first 4 words\n",
    "    word = gram[-1]       # predicted word\n",
    "    context_to_candidates[context].add(word)\n",
    "\n",
    "def generate_text_max_prob(seed_text, num_words=50):\n",
    "    seed_tokens = re.findall(r\"\\b\\w+\\b|[.,!?;]\", seed_text.lower())\n",
    "\n",
    "    if len(seed_tokens) < 4:\n",
    "        raise ValueError(\"Seed text must contain at least 4 words\")\n",
    "\n",
    "    generated = seed_tokens[:]\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        context = tuple(generated[-4:])\n",
    "\n",
    "        if context not in context_to_candidates:\n",
    "            break\n",
    "\n",
    "        # Find word with highest probability\n",
    "        best_word = None\n",
    "        best_prob = 0.0\n",
    "\n",
    "        for word in context_to_candidates[context]:\n",
    "            prob = fivegram_probability(context, word)\n",
    "\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_word = word\n",
    "\n",
    "        if best_word is None:\n",
    "            break\n",
    "\n",
    "        generated.append(best_word)\n",
    "\n",
    "    return \" \".join(generated)\n",
    "\n",
    "print(generate_text_max_prob(\"we arrived at baker street\", 40))\n",
    "print(generate_text_max_prob(\"holmes looked at me\", 40))\n",
    "print(generate_text_max_prob(\"i could not believe\", 40))\n",
    "print(generate_text_max_prob(\"he said nothing but smiled\", 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be952b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadac8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
