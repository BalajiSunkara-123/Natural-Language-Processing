{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:56:56.127047Z",
     "iopub.status.busy": "2026-01-12T11:56:56.126380Z",
     "iopub.status.idle": "2026-01-12T11:56:57.362182Z",
     "shell.execute_reply": "2026-01-12T11:56:57.361616Z",
     "shell.execute_reply.started": "2026-01-12T11:56:56.127016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Collecting data\n",
    "file=open(\"/kaggle/input/wikipedia-dump/enwik8\",\"r\")\n",
    "text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:56:58.470941Z",
     "iopub.status.busy": "2026-01-12T11:56:58.470193Z",
     "iopub.status.idle": "2026-01-12T11:57:05.731914Z",
     "shell.execute_reply": "2026-01-12T11:57:05.731288Z",
     "shell.execute_reply.started": "2026-01-12T11:56:58.470910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikipedia', 'http', 'en', 'wikipedia', 'org', 'wiki', 'main', 'page', 'mediawiki', 'alpha']\n"
     ]
    }
   ],
   "source": [
    "# removing all the unnecessary data like tags...\n",
    "import re\n",
    "text=re.sub(r\"<[^>]+>\",\" \",text)\n",
    "text=text.lower()\n",
    "text=re.sub(r\"[^a-z\\s]\",\" \",text)\n",
    "tokens=text.split()\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Construction \n",
    "* Neural Networks cannot work on strings and rare words adds noise\n",
    "* So we will apply minimum frequency cutoff i.e rare words are removed from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:05.733366Z",
     "iopub.status.busy": "2026-01-12T11:57:05.733094Z",
     "iopub.status.idle": "2026-01-12T11:57:09.906797Z",
     "shell.execute_reply": "2026-01-12T11:57:09.906086Z",
     "shell.execute_reply.started": "2026-01-12T11:57:05.733342Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 71993\n"
     ]
    }
   ],
   "source": [
    "freq={}\n",
    "min_count=5\n",
    "word2id={}\n",
    "id2word={}\n",
    "\n",
    "for token in tokens:\n",
    "    if token in freq.keys():\n",
    "        freq[token]+=1\n",
    "    else:\n",
    "        freq[token]=1\n",
    "sorted_freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "idx=0\n",
    "for word,count in sorted_freq:\n",
    "    if count>=min_count:\n",
    "        word2id[word]=idx\n",
    "        id2word[idx]=word\n",
    "        idx+=1\n",
    "vocab_size = len(word2id)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:09.907977Z",
     "iopub.status.busy": "2026-01-12T11:57:09.907723Z",
     "iopub.status.idle": "2026-01-12T11:57:11.899153Z",
     "shell.execute_reply": "2026-01-12T11:57:11.898561Z",
     "shell.execute_reply.started": "2026-01-12T11:57:09.907955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_size=4\n",
    "word_indices = [word2id[token] for token in tokens if token in word2id]\n",
    "\n",
    "#Decreasing the size for time constraints\n",
    "word_indices = word_indices[:20000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Unigram Probabilites for Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:11.901195Z",
     "iopub.status.busy": "2026-01-12T11:57:11.900651Z",
     "iopub.status.idle": "2026-01-12T11:57:11.953594Z",
     "shell.execute_reply": "2026-01-12T11:57:11.953030Z",
     "shell.execute_reply.started": "2026-01-12T11:57:11.901169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram prob sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Unigram^0.75 distribution over VOCAB ONLY\n",
    "unigram_words = []\n",
    "unigram_probs = []\n",
    "\n",
    "for word, idx in word2id.items():\n",
    "    unigram_words.append(idx)\n",
    "    unigram_probs.append(freq[word] ** 0.75)\n",
    "\n",
    "# Normalize\n",
    "total = sum(unigram_probs)\n",
    "unigram_probs = [p / total for p in unigram_probs]\n",
    "\n",
    "print(\"unigram prob sum:\", sum(unigram_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:11.954800Z",
     "iopub.status.busy": "2026-01-12T11:57:11.954549Z",
     "iopub.status.idle": "2026-01-12T11:57:11.969867Z",
     "shell.execute_reply": "2026-01-12T11:57:11.969305Z",
     "shell.execute_reply.started": "2026-01-12T11:57:11.954759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, center_word, pos_context, neg_contexts):\n",
    "        w = self.target_embeddings(center_word)        # (1, d)\n",
    "        c_pos = self.context_embeddings(pos_context)   # (1, d)\n",
    "        c_neg = self.context_embeddings(neg_contexts)  # (k, d)\n",
    "\n",
    "        # Positive loss\n",
    "        pos_score = torch.sum(w * c_pos, dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "        # Negative loss\n",
    "        neg_score = torch.matmul(c_neg, w.t()).squeeze()\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score)))\n",
    "\n",
    "        return pos_loss + neg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:12.225841Z",
     "iopub.status.busy": "2026-01-12T11:57:12.225599Z",
     "iopub.status.idle": "2026-01-12T11:57:12.230061Z",
     "shell.execute_reply": "2026-01-12T11:57:12.229450Z",
     "shell.execute_reply.started": "2026-01-12T11:57:12.225819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_negatives(k, unigram_words, unigram_probs, forbidden):\n",
    "    negatives = []\n",
    "    while len(negatives) < k:\n",
    "        neg_id = random.choices(unigram_words, unigram_probs)[0]\n",
    "        if neg_id not in forbidden:\n",
    "            negatives.append(neg_id)\n",
    "    return negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:15.160428Z",
     "iopub.status.busy": "2026-01-12T11:57:15.160128Z",
     "iopub.status.idle": "2026-01-12T11:57:15.326792Z",
     "shell.execute_reply": "2026-01-12T11:57:15.326196Z",
     "shell.execute_reply.started": "2026-01-12T11:57:15.160402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_negatives = 5\n",
    "learning_rate = 0.025\n",
    "epochs = 1\n",
    "\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-12T11:56:53.197754Z",
     "iopub.status.idle": "2026-01-12T11:56:53.197968Z",
     "shell.execute_reply": "2026-01-12T11:56:53.197875Z",
     "shell.execute_reply.started": "2026-01-12T11:56:53.197861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "window_size = 4\n",
    "total_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "\n",
    "    for i in tqdm(range(len(word_indices))):\n",
    "        center_word = word_indices[i]\n",
    "\n",
    "        for j in range(1, window_size // 2 + 1):\n",
    "\n",
    "            # Left context\n",
    "            if i - j >= 0:\n",
    "                context_word = word_indices[i - j]\n",
    "\n",
    "                negatives = sample_negatives(\n",
    "                    num_negatives,\n",
    "                    unigram_words,\n",
    "                    unigram_probs,\n",
    "                    forbidden={center_word, context_word}\n",
    "                )\n",
    "\n",
    "                center = torch.tensor([center_word])\n",
    "                pos = torch.tensor([context_word])\n",
    "                neg = torch.tensor(negatives)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(center, pos, neg)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Right context\n",
    "            if i + j < len(word_indices):\n",
    "                context_word = word_indices[i + j]\n",
    "\n",
    "                negatives = sample_negatives(\n",
    "                    num_negatives,\n",
    "                    unigram_words,\n",
    "                    unigram_probs,\n",
    "                    forbidden={center_word, context_word}\n",
    "                )\n",
    "\n",
    "                center = torch.tensor([center_word])\n",
    "                pos = torch.tensor([context_word])\n",
    "                neg = torch.tensor(negatives)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(center, pos, neg)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    print(\"Total loss:\", total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:18.140866Z",
     "iopub.status.busy": "2026-01-12T11:57:18.140154Z",
     "iopub.status.idle": "2026-01-12T11:57:18.164772Z",
     "shell.execute_reply": "2026-01-12T11:57:18.164161Z",
     "shell.execute_reply.started": "2026-01-12T11:57:18.140837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use target embeddings (W)\n",
    "embeddings = model.target_embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "np.save(\"my_word2vec_embeddings.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:20.045664Z",
     "iopub.status.busy": "2026-01-12T11:57:20.044970Z",
     "iopub.status.idle": "2026-01-12T11:57:20.067799Z",
     "shell.execute_reply": "2026-01-12T11:57:20.067043Z",
     "shell.execute_reply.started": "2026-01-12T11:57:20.045634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"word2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:21.565873Z",
     "iopub.status.busy": "2026-01-12T11:57:21.565371Z",
     "iopub.status.idle": "2026-01-12T11:57:21.572142Z",
     "shell.execute_reply": "2026-01-12T11:57:21.571250Z",
     "shell.execute_reply.started": "2026-01-12T11:57:21.565846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - queen : -0.0263\n",
      "man - woman : 0.0451\n",
      "paris - france : 0.0788\n",
      "delhi - india : 0.1913\n",
      "computer - laptop : -0.0204\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2) + 1e-9)\n",
    "def get_vector(word):\n",
    "    if word not in word2id:\n",
    "        return None\n",
    "    return embeddings[word2id[word]]\n",
    "pairs = [\n",
    "    (\"king\", \"queen\"),\n",
    "    (\"man\", \"woman\"),\n",
    "    (\"paris\", \"france\"),\n",
    "    (\"delhi\", \"india\"),\n",
    "    (\"computer\", \"laptop\")\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    v1 = get_vector(w1)\n",
    "    v2 = get_vector(w2)\n",
    "    if v1 is not None and v2 is not None:\n",
    "        print(f\"{w1} - {w2} : {cosine_similarity(v1, v2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:57:34.089200Z",
     "iopub.status.busy": "2026-01-12T11:57:34.088898Z",
     "iopub.status.idle": "2026-01-12T11:57:35.113149Z",
     "shell.execute_reply": "2026-01-12T11:57:35.112367Z",
     "shell.execute_reply.started": "2026-01-12T11:57:34.089173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman:\n",
      "woman\n",
      "\n",
      "paris - france + india:\n",
      "india\n"
     ]
    }
   ],
   "source": [
    "def analogy(a, b, c, top_k=5):\n",
    "    if a not in word2id or b not in word2id or c not in word2id:\n",
    "        return None\n",
    "\n",
    "    target_vec = embeddings[word2id[a]] - embeddings[word2id[b]] + embeddings[word2id[c]]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(embeddings)):\n",
    "        score = cosine_similarity(target_vec, embeddings[i])\n",
    "        scores.append((id2word[i], score))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n",
    "\n",
    "print(\"king - man + woman:\")\n",
    "print(analogy(\"king\", \"man\", \"woman\")[0][0])\n",
    "\n",
    "print(\"\\nparis - france + india:\")\n",
    "print(analogy(\"paris\", \"france\", \"india\")[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:58:00.941576Z",
     "iopub.status.busy": "2026-01-12T11:58:00.940897Z",
     "iopub.status.idle": "2026-01-12T11:58:00.948128Z",
     "shell.execute_reply": "2026-01-12T11:58:00.947246Z",
     "shell.execute_reply.started": "2026-01-12T11:58:00.941545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor     -> -0.0773\n",
      "nurse      -> 0.0402\n",
      "engineer   -> -0.1015\n",
      "teacher    -> 0.0137\n",
      "scientist  -> -0.2043\n"
     ]
    }
   ],
   "source": [
    "male_words = [\"he\", \"man\", \"male\"]\n",
    "female_words = [\"she\", \"woman\", \"female\"]\n",
    "\n",
    "def mean_vector(words):\n",
    "    vecs = [get_vector(w) for w in words if get_vector(w) is not None]\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "gender_direction = mean_vector(male_words) - mean_vector(female_words)\n",
    "occupations = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"scientist\"]\n",
    "\n",
    "for word in occupations:\n",
    "    vec = get_vector(word)\n",
    "    if vec is not None:\n",
    "        score = cosine_similarity(vec, gender_direction)\n",
    "        print(f\"{word:10s} -> {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:58:43.997086Z",
     "iopub.status.busy": "2026-01-12T11:58:43.996535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "gensim_model = api.load(\"word2vec-google-news-300\")\n",
    "for w1, w2 in pairs:\n",
    "    if w1 in gensim_model and w2 in gensim_model:\n",
    "        gensim_sim = gensim_model.similarity(w1, w2)\n",
    "        my_sim = cosine_similarity(get_vector(w1), get_vector(w2))\n",
    "        print(f\"{w1:10s} {w2:10s} | My: {my_sim:.4f} | Gensim: {gensim_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9210249,
     "sourceId": 14420392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
